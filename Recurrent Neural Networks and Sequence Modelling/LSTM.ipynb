{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9b367c",
   "metadata": {},
   "source": [
    "### Long Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca96c4",
   "metadata": {},
   "source": [
    "Introduction to LSTMs and How They Address RNN Limitations\n",
    "---\n",
    "\n",
    "### What are LSTMs?\n",
    "\n",
    "Long Short-Term Memory networks (LSTMs) are a specialized type of recurrent neural network (RNN) designed to effectively capture long-term dependencies in sequential data. While traditional RNNs are theoretically capable of handling sequences of arbitrary length, in practice they struggle with learning patterns that span many time steps due to the vanishing gradient problem. This issue makes it difficult for standard RNNs to retain information over long sequences. LSTMs address this limitation by introducing a memory cell and gating mechanisms that regulate the flow of information, enabling the network to remember or forget information as needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of LSTMs\n",
    "\n",
    "- **Memory Cells:**  \n",
    "    LSTMs maintain a dedicated memory cell at each time step, which acts as a conveyor belt for information. This cell state is modified only through carefully regulated gates, allowing the network to preserve information across long sequences.\n",
    "\n",
    "- **Gated Mechanism:**  \n",
    "    LSTMs use three primary gates—forget, input, and output—to control the flow of information:\n",
    "        - The **forget gate** decides what information to discard from the cell state.\n",
    "        - The **input gate** determines which new information to add.\n",
    "        - The **output gate** controls what information from the cell state is output as the hidden state.\n",
    "    This selective memory management is crucial for learning complex temporal patterns.\n",
    "\n",
    "- **Effective for Long Sequences:**  \n",
    "    By mitigating the vanishing gradient problem, LSTMs can learn dependencies that span many time steps, making them suitable for tasks like language modeling, time series forecasting, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages Over Vanilla RNNs\n",
    "\n",
    "- **Retain Long-Term Dependencies:**  \n",
    "    LSTMs are explicitly designed to remember information for long durations, overcoming the limitations of standard RNNs.\n",
    "\n",
    "- **Prevents Gradient-Related Issues:**  \n",
    "    The gating mechanisms help prevent both vanishing and exploding gradients during training, leading to more stable and effective learning.\n",
    "\n",
    "- **Superior Performance:**  \n",
    "    LSTMs consistently outperform vanilla RNNs on tasks such as language modeling, speech recognition, and time series prediction due to their ability to model long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### LSTM Cell Structure: Input, Forget, and Output Gates\n",
    "\n",
    "A typical LSTM cell consists of three main gates that control the flow of information:\n",
    "\n",
    "- **Forget Gate (\\(f_t\\)):**  \n",
    "    Decides what information from the previous cell state should be discarded. It takes the previous hidden state and the current input, passes them through a sigmoid activation, and outputs a value between 0 and 1 for each element in the cell state (0 = \"completely forget\", 1 = \"completely keep\").\n",
    "\n",
    "- **Input Gate (\\(i_t\\)):**  \n",
    "    Determines which new information should be added to the cell state. It uses a sigmoid layer to decide which values to update and a tanh layer to create a vector of new candidate values (\\(\\tilde{C}_t\\)).\n",
    "\n",
    "- **Cell State Update (\\(C_t\\)):**  \n",
    "    The cell state is updated by combining the results of the forget and input gates, allowing the network to selectively remember or forget information.\n",
    "\n",
    "- **Output Gate (\\(o_t\\)):**  \n",
    "    Controls what part of the cell state should be output as the hidden state for the next time step. It uses a sigmoid layer to decide which parts of the cell state to output, and a tanh layer to scale the cell state values.\n",
    "\n",
    "---\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Let  \n",
    "- \\( x_t \\): input at time step \\( t \\)  \n",
    "- \\( h_{t-1} \\): previous hidden state  \n",
    "- \\( C_{t-1} \\): previous cell state  \n",
    "- \\( W \\), \\( U \\), \\( b \\): weight matrices and biases  \n",
    "- \\( \\sigma \\): sigmoid activation  \n",
    "- \\( \\tanh \\): hyperbolic tangent activation  \n",
    "\n",
    "The LSTM cell computes:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "f_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\quad &\\text{(Forget gate)} \\\\\n",
    "i_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\quad &\\text{(Input gate)} \\\\\n",
    "\\tilde{C}_t &= \\tanh(W_C x_t + U_C h_{t-1} + b_C) \\quad &\\text{(Candidate cell state)} \\\\\n",
    "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\quad &\\text{(New cell state)} \\\\\n",
    "o_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\quad &\\text{(Output gate)} \\\\\n",
    "h_t &= o_t \\odot \\tanh(C_t) \\quad &\\text{(New hidden state)}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\( f_t \\): forget gate vector  \n",
    "- \\( i_t \\): input gate vector  \n",
    "- \\( o_t \\): output gate vector  \n",
    "- \\( \\tilde{C}_t \\): candidate values for cell state  \n",
    "- \\( C_t \\): updated cell state  \n",
    "- \\( h_t \\): updated hidden state  \n",
    "- \\( \\odot \\): element-wise multiplication  \n",
    "\n",
    "---\n",
    "\n",
    "#### Summary Table\n",
    "\n",
    "| Gate         | Purpose                                      | Activation Functions |\n",
    "|--------------|----------------------------------------------|----------------------|\n",
    "| Forget Gate  | Remove irrelevant information                | Sigmoid              |\n",
    "| Input Gate   | Add new relevant information                 | Sigmoid, Tanh        |\n",
    "| Output Gate  | Output filtered cell state as hidden state   | Sigmoid, Tanh        |\n",
    "\n",
    "This gating mechanism allows LSTMs to selectively remember or forget information, making them highly effective for learning long-term dependencies in sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of LSTMs\n",
    "\n",
    "LSTMs have been widely adopted in various domains due to their ability to model sequential data and capture long-term dependencies. Some notable applications include:\n",
    "\n",
    "- **Natural Language Processing (NLP):**  \n",
    "    - *Sentiment Analysis:* Understanding the sentiment of a sentence or document by considering the context provided by previous words.\n",
    "    - *Machine Translation:* Translating text from one language to another by capturing the context of entire sentences or paragraphs.\n",
    "    - *Text Generation:* Generating coherent and contextually relevant text sequences, such as chatbots or story generation.\n",
    "\n",
    "- **Time Series Forecasting:**  \n",
    "    - *Stock Price Prediction:* Modeling and predicting future stock prices based on historical data.\n",
    "    - *Weather Forecasting:* Predicting future weather patterns by analyzing past meteorological data.\n",
    "    - *Sales Trends:* Forecasting future sales based on previous sales data and seasonal trends.\n",
    "\n",
    "- **Speech Recognition:**  \n",
    "    - *Speech-to-Text Conversion:* Converting spoken words into written text by analyzing audio signals over time.\n",
    "\n",
    "- **Anomaly Detection:**  \n",
    "    - *Identifying Unusual Patterns:* Detecting anomalies in sequential data, such as fraud detection in financial transactions or fault detection in industrial systems.\n",
    "\n",
    "LSTMs' ability to capture both short-term and long-term dependencies makes them a powerful tool for a wide range of sequence modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2529a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2323f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (25000, 200)\n",
      "Testing Data Shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "vocab_size = 10000\n",
    "max_len = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Testing Data Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22e6ab",
   "metadata": {},
   "source": [
    "train and build basic rnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2c9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step - accuracy: 0.5469 - loss: 0.6817 - val_accuracy: 0.6088 - val_loss: 0.6416\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 28ms/step - accuracy: 0.7151 - loss: 0.5597 - val_accuracy: 0.7584 - val_loss: 0.5109\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 29ms/step - accuracy: 0.7639 - loss: 0.4949 - val_accuracy: 0.6828 - val_loss: 0.6026\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step - accuracy: 0.8041 - loss: 0.4387 - val_accuracy: 0.7852 - val_loss: 0.5116\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 27ms/step - accuracy: 0.8706 - loss: 0.3196 - val_accuracy: 0.8142 - val_loss: 0.4799\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.8049 - loss: 0.4823\n",
      "Test Loss: 0.4771870970726013, Test Accuracy: 0.8063600063323975\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        SimpleRNN(128, activation=\"tanh\", return_sequences=False),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "rnn_model.summary()\n",
    "\n",
    "rnn_history = rnn_model.fit(\n",
    "    X_train, y_train, epochs=5, batch_size=32, validation_split=0.2\n",
    ")\n",
    "\n",
    "loss, accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c916e",
   "metadata": {},
   "source": [
    "create lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e01fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 51ms/step - accuracy: 0.7374 - loss: 0.5141 - val_accuracy: 0.6494 - val_loss: 0.6184\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 50ms/step - accuracy: 0.7948 - loss: 0.4437 - val_accuracy: 0.8024 - val_loss: 0.4261\n",
      "Epoch 3/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 50ms/step - accuracy: 0.8933 - loss: 0.2693 - val_accuracy: 0.7686 - val_loss: 0.4876\n",
      "Epoch 4/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 53ms/step - accuracy: 0.8893 - loss: 0.2820 - val_accuracy: 0.8596 - val_loss: 0.3700\n",
      "Epoch 5/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 53ms/step - accuracy: 0.9501 - loss: 0.1442 - val_accuracy: 0.8732 - val_loss: 0.3605\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.8627 - loss: 0.3862\n",
      "LSTM Test Loss: 0.38313448429107666, Test Accuracy: 0.8621199727058411\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        LSTM(128, activation=\"tanh\", return_sequences=False),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lstm_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train, y_train, epochs=5, batch_size=32, validation_split=0.2\n",
    ")\n",
    "\n",
    "loss, accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"LSTM Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
