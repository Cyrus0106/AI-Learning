{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca80b32",
   "metadata": {},
   "source": [
    "### Sequence to sequence models and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5b484",
   "metadata": {},
   "source": [
    "## Seq2Seq Models and Their Architectures\n",
    "\n",
    "### What are Seq2Seq Models?\n",
    "\n",
    "Sequence-to-sequence (Seq2Seq) models are neural network architectures designed to map an input sequence to an output sequence, which may be of a different length. They are widely used for tasks such as:\n",
    "\n",
    "- Machine translation (e.g., English to French)\n",
    "- Text summarization\n",
    "- Speech-to-text conversion\n",
    "- Chatbots and conversational agents\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "#### Encoder-Decoder Framework\n",
    "\n",
    "- **Encoder:**  \n",
    "    Processes the input sequence (e.g., a sentence) token by token using RNN, LSTM, or GRU layers.  \n",
    "    The encoder summarizes the entire input sequence into a fixed-length vector, often called the **context vector** or **thought vector**.\n",
    "\n",
    "- **Decoder:**  \n",
    "    Takes the context vector as its initial hidden state and generates the output sequence one token at a time.  \n",
    "    At each step, the decoder predicts the next token based on the context vector and the tokens generated so far.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. **Encoder:**\n",
    "     - Sequentially processes the input sequence.\n",
    "     - Updates its hidden state at each time step.\n",
    "     - Outputs the final hidden state (context vector) representing the entire input.\n",
    "\n",
    "2. **Decoder:**\n",
    "     - Initializes its hidden state with the encoder's context vector.\n",
    "     - Generates the output sequence token by token.\n",
    "     - At each step, predicts the next token using the previous tokens and its current hidden state.\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Mechanism Overview\n",
    "\n",
    "#### Why Attention?\n",
    "\n",
    "- Standard Seq2Seq models compress the entire input sequence into a single fixed-length vector, which can lead to information loss, especially for long sequences.\n",
    "- The **attention mechanism** allows the model to focus on different parts of the input sequence at each decoding step, improving performance on complex tasks.\n",
    "\n",
    "#### How Attention Works\n",
    "\n",
    "- At each decoding step, the decoder calculates a **weight (or score)** for each input token, reflecting its relevance to the current decoding context.\n",
    "- These weights are used to compute a **weighted sum** of the encoder outputs, producing a dynamic context vector for each output token.\n",
    "- This enables the decoder to \"attend\" to different parts of the input sequence as needed.\n",
    "\n",
    "#### Benefits of Attention\n",
    "\n",
    "- Helps the model handle long input sequences more effectively.\n",
    "- Improves translation quality, summarization, and other sequence generation tasks.\n",
    "- Forms the basis for advanced architectures like the Transformer.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Seq2Seq models, especially with attention mechanisms, are foundational for many NLP tasks. They enable flexible mapping between sequences of varying lengths and have inspired modern architectures such as Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a51e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Example english to french sentences\n",
    "english_sentences = [\"hello\", \"how are you\", \"good morning\", \"thank you\", \"good night\"]\n",
    "french_sentences = [\"bonjour\", \"comment Ã§a va\", \"bon matin\", \"merci\", \"bonne nuit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404762d",
   "metadata": {},
   "source": [
    "vocabulary and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd33667",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'printing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    100\u001b[39m decoder = Decoder(output_dim, embed_dim, hidden_dim, num_layers)\n\u001b[32m    101\u001b[39m model = Seq2Seq(encoder, decoder, device).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m optimizer = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m criterion = nn.CrossEntropyLoss(ignore_index = french_vocab[\u001b[33m\"\u001b[39m\u001b[33m<PAD>\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# train model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:100\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     87\u001b[39m defaults = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     88\u001b[39m     lr=lr,\n\u001b[32m     89\u001b[39m     betas=betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m     decoupled_weight_decay=decoupled_weight_decay,\n\u001b[32m     99\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:369\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    366\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, convert_frame, eval_frame, resume_execution\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m guard_bool\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_dynamo\\exc.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     Application,\n\u001b[32m     69\u001b[39m     CeilToInt,\n\u001b[32m     70\u001b[39m     CleanDiv,\n\u001b[32m     71\u001b[39m     FloorDiv,\n\u001b[32m     72\u001b[39m     FloorToInt,\n\u001b[32m     73\u001b[39m     IsNonOverlappingAndDenseIndicator,\n\u001b[32m     74\u001b[39m     Max,\n\u001b[32m     75\u001b[39m     Mod,\n\u001b[32m     76\u001b[39m     PythonMod,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CppPrinter, PythonPrinter\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:183\u001b[39m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gcd  \u001b[38;5;66;03m# type: ignore[return-value]  # remove in py3.12\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# It would be nice to have assertions on whether or not inputs is_integer\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# However, with bugs like https://github.com/sympy/sympy/issues/26620 sympy\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# sometimes inconsistently reports floats an integers.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# not, this can potentially cause correctness issues.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mFloorDiv\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43msympy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"\u001b[39;49;00m\n\u001b[32m    185\u001b[39m \u001b[33;43;03m    We maintain this so that:\u001b[39;49;00m\n\u001b[32m    186\u001b[39m \u001b[33;43;03m    1. We can use divisibility guards to simplify FloorDiv(a, b) to a / b.\u001b[39;49;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33;43;03m    NB: This is Python-style floor division, round to -Inf\u001b[39;49;00m\n\u001b[32m    190\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:204\u001b[39m, in \u001b[36mFloorDiv\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdivisor\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> sympy.Basic:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sympystr\u001b[39m(\u001b[38;5;28mself\u001b[39m, printer: \u001b[43msympy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprinting\u001b[49m.StrPrinter) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    205\u001b[39m     base = printer.parenthesize(\u001b[38;5;28mself\u001b[39m.base, PRECEDENCE[\u001b[33m\"\u001b[39m\u001b[33mAtom\u001b[39m\u001b[33m\"\u001b[39m] - \u001b[32m0.5\u001b[39m)\n\u001b[32m    206\u001b[39m     divisor = printer.parenthesize(\u001b[38;5;28mself\u001b[39m.divisor, PRECEDENCE[\u001b[33m\"\u001b[39m\u001b[33mAtom\u001b[39m\u001b[33m\"\u001b[39m] - \u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'sympy' has no attribute 'printing'"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences):\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "english_vocab = build_vocab(english_sentences)\n",
    "french_vocab  = build_vocab(french_sentences)\n",
    "\n",
    "# tokenize and pad sentences\n",
    "def tokenize(sentences, vocab, max_len):\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [vocab.get(word, vocab['<UNK>']) for word in sentence.split()]\n",
    "        tokens = [vocab[\"<SOS>\"]] + tokens + [vocab[\"<EOS>\"]]\n",
    "        tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
    "        tokenized.append(tokens)\n",
    "    return np.array(tokenized)\n",
    "\n",
    "max_len_eng = max(len(sentence.split()) for sentence in english_sentences)\n",
    "max_len_fr = max(len(sentence.split()) for sentence in french_sentences)\n",
    "\n",
    "english_data = tokenize(english_sentences, english_vocab, max_len_eng)\n",
    "french_data = tokenize(french_sentences, french_vocab, max_len_fr)\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.src_data[index]), torch.tensor(self.tgt_data[index])\n",
    "    \n",
    "dataset = TranslationDataset(english_data,french_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle = True)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim,num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim,num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x , hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden,cell) = self.lstm(embedded,(hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size,tgt_len, tgt_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = tgt[:,0]\n",
    "\n",
    "        for t in range(1,tgt_len):\n",
    "            output,hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:,t,:] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:,t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = len(english_vocab)\n",
    "output_dim = len(french_vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers =2 \n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(output_dim, embed_dim, hidden_dim, num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = french_vocab[\"<PAD>\"])\n",
    "\n",
    "# train model\n",
    "def train(model, dataloader, optimizer, criterion, device, num_epochs =20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src,tgt)\n",
    "            output = output[:,1:].reshape(-1, output.shape[2])\n",
    "            tgt = tgt[:,1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1},/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n",
    "\n",
    "\n",
    "train(model,dataloader, optimizer, criterion, device)\n",
    "\n",
    "def translate_sentence(model, sentence, english_vocab, french_vocab, max_len_fr, device):\n",
    "    model.eval()\n",
    "    tokens = [english_vocab.get(word, english_vocab[\"<UNK>\"]) for word in sentence.split()]\n",
    "    tokens = [english_vocab[\"<SOS>\"]] + tokens + [english_vocab[\"<EOS>\"]]\n",
    "    src = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src)\n",
    "\n",
    "    tgt_vocab = {v:k for k, v in french_vocab.items()}\n",
    "    tgt_indices = [french_vocab[\"<SOS>\"]]\n",
    "    for _ in range(max_len_fr):\n",
    "        tgt_tensor = torch.tensor([tgt_indices[-1]]).to(device)\n",
    "        output, hidden, cell = model.decoder(tgt_tensor, hidden, cell)\n",
    "        pred = output.argmax(1).item()\n",
    "        tgt_indices.append(pred)\n",
    "        if pred == french_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "    translated_sentence = [tgt_vocab[i] for i in tgt_indices[1:-1]]\n",
    "    return \" \".join(translated_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f169c2",
   "metadata": {},
   "source": [
    "test translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d8604",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"good morning\"\n",
    "translation = translate_sentence(model, sentence, english_vocab, french_vocab, max_len_fr, device)\n",
    "\n",
    "print(f\"{sentence} -> {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
