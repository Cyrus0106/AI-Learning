{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149eb8e6",
   "metadata": {},
   "source": [
    "### Transformer Project - Text Summarisation or Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae255b",
   "metadata": {},
   "source": [
    "## Applying Transformer-Based Models to Advanced NLP Tasks\n",
    "\n",
    "### Text Summarisation\n",
    "\n",
    "Text summarisation is the process of condensing a piece of text while retaining its key information. There are two main types:\n",
    "\n",
    "- **Extractive Summarisation:** Selects key phrases or sentences directly from the original text to create a summary.\n",
    "- **Abstractive Summarisation:** Generates new sentences that capture the meaning of the original text, often rephrasing or paraphrasing content.\n",
    "\n",
    "**Applications:**\n",
    "- News article summarisation\n",
    "- Document and report summarisation\n",
    "- Meeting transcript summarisation\n",
    "\n",
    "---\n",
    "\n",
    "### Text Translation\n",
    "\n",
    "Text translation involves converting text from one language to another while maintaining meaning, context, and grammatical correctness.\n",
    "\n",
    "**Examples:**\n",
    "- English to French translation\n",
    "- Multi-lingual translations using models like T5 or mT5\n",
    "- Real-time translation in chat applications\n",
    "\n",
    "---\n",
    "\n",
    "### Fine-Tuning and Optimising Models\n",
    "\n",
    "#### Pre-Trained Models for Summarisation and Translation\n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer):** Treats every NLP problem as a text-to-text task. Can be fine-tuned for both summarisation and translation tasks.\n",
    "- **BART (Bidirectional and Auto-Regressive Transformer):** Combines a BERT-like encoder and a GPT-like decoder. Pre-trained for denoising and can be fine-tuned for summarisation and translation.\n",
    "\n",
    "#### Optimisation Techniques\n",
    "\n",
    "- **Learning Rate Scheduling:** Adjusts the learning rate during training to improve convergence.\n",
    "- **Hyperparameter Tuning:** Involves adjusting parameters such as batch size, optimiser type, and maximum sequence length to achieve optimal performance.\n",
    "- **Early Stopping:** Stops training when performance on a validation set stops improving.\n",
    "- **Data Augmentation:** Increases the diversity of training data to improve model robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### Analysing Model Performance\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- **Text Summarisation:**\n",
    "    - **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures overlap between generated and reference summaries.\n",
    "    - **BLEU (Bilingual Evaluation Understudy):** Originally for translation, but sometimes used for summarisation to measure n-gram overlap.\n",
    "- **Text Translation:**\n",
    "    - **BLEU Score:** Evaluates translation quality by comparing n-grams of the candidate and reference translations.\n",
    "    - **Perplexity:** Measures how well a probability model predicts a sample, indicating model confidence.\n",
    "    - **METEOR, TER:** Additional metrics for translation quality.\n",
    "\n",
    "#### Additional Considerations\n",
    "\n",
    "- **Human Evaluation:** Involves human judges rating the quality and fluency of generated summaries or translations.\n",
    "- **Error Analysis:** Identifying common failure cases to guide further model improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ebde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datset for summarisation\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# tokenise for summarisation\n",
    "def tokenize_funtion(examples):\n",
    "    inputs = [\"Summarise: \"+ doc for doc in examples [\"articles\"]]\n",
    "    model_inputs = tokenizer(inputs, max_lenght=512, truncation=True)\n",
    "\n",
    "    # tokenise targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlights\"], max_length=150, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_funtion,batched=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit= 2,\n",
    "    predict_with_generate = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"validation\"],\n",
    "    processing_class= tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "sample_text = \"The Transformer model revolutionised NLP by enabling parallel processing of sequences\"\n",
    "inputs = tokenizer(\"summarize:\"+ sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(input[\"input_ids\"], max_length=150, num_beams=4, early_stopping= True)\n",
    "\n",
    "print(\"Original text:\\n\", sample_text)\n",
    "print(\"Summary:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "metric = load_metric(\"rogue\")\n",
    "predictions = outputs[\"generated_text\"]\n",
    "references = dataset[\"validation\"][\"highlights\"]\n",
    "\n",
    "results = metric.compute(prediction = predictions , references=references)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
