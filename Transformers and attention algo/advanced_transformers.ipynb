{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff41e24c",
   "metadata": {},
   "source": [
    "### Advanced Transformers BERT Variants and GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6f73a",
   "metadata": {},
   "source": [
    "## Exploration of BERT Variants\n",
    "\n",
    "### Why BERT Variants?\n",
    "While BERT is a powerful transformer-based model, it has some limitations:\n",
    "- **Large computational requirements:** BERT's size and architecture make it resource-intensive, limiting its deployment in real-time or resource-constrained environments.\n",
    "- **Inefficiencies in capturing certain nuances:** The original BERT architecture may not optimally handle all types of language tasks or domain-specific data.\n",
    "\n",
    "**BERT variants** have been developed to:\n",
    "- Optimize the model for specific tasks or domains\n",
    "- Improve performance on downstream tasks\n",
    "- Reduce computational overhead and memory usage\n",
    "\n",
    "---\n",
    "\n",
    "### Key BERT Variants\n",
    "\n",
    "- **RoBERTa (Robustly Optimized BERT Approach):**\n",
    "    - Removes the Next Sentence Prediction (NSP) task for better efficiency.\n",
    "    - Trains on more data with larger batch sizes and longer sequences.\n",
    "    - Uses dynamic masking during training.\n",
    "    - **Use Case:** Superior performance in tasks requiring deeper context understanding, such as reading comprehension and sentiment analysis.\n",
    "\n",
    "- **DistilBERT:**\n",
    "    - A distilled (smaller and faster) version of BERT.\n",
    "    - Retains ~97% of BERT's performance while being 60% faster and 40% smaller.\n",
    "    - Achieves efficiency through knowledge distillation.\n",
    "    - **Use Case:** Ideal for real-time applications and deployment on devices with limited resources.\n",
    "\n",
    "- **ALBERT (A Lite BERT):**\n",
    "    - Reduces memory consumption by factorizing embeddings and sharing parameters across layers.\n",
    "    - Achieves similar or better performance with fewer parameters.\n",
    "    - **Use Case:** Suitable for large-scale pre-training and downstream tasks with memory limitations.\n",
    "\n",
    "- **BERTweet:**\n",
    "    - Fine-tuned on large-scale English Twitter data.\n",
    "    - Specialized for social media text, handling informal language, hashtags, and emojis.\n",
    "    - **Use Case:** Social media sentiment analysis, hashtag prediction, and other Twitter-specific NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to GPT-3\n",
    "\n",
    "### What is GPT-3?\n",
    "- Developed by **OpenAI**.\n",
    "- A massive language model with **175 billion parameters** trained on diverse internet-scale datasets.\n",
    "- Excels at generating coherent, contextually relevant, and human-like text.\n",
    "\n",
    "### Key Features of GPT-3\n",
    "- **Zero-shot and few-shot learning:** Can perform new tasks with minimal or no fine-tuning by conditioning on prompts.\n",
    "- **Versatility:** Used for text generation, summarization, question answering, translation, code generation, and conversational AI.\n",
    "- **Contextual understanding:** Maintains context over long passages, enabling more natural conversations and content creation.\n",
    "\n",
    "### Applications\n",
    "- **Conversational AI:** Chatbots, virtual assistants, and customer support.\n",
    "- **Content Generation:** Articles, scripts, marketing copy, and code snippets.\n",
    "- **Creative Writing:** Poems, stories, brainstorming, and ideation.\n",
    "- **Education and Tutoring:** Automated explanations, question generation, and personalized learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Transfer Learning in NLP with Transformer Models\n",
    "\n",
    "### What is Transfer Learning?\n",
    "Transfer learning involves:\n",
    "- Pre-training a model on a large, general-purpose dataset (e.g., Wikipedia, BookCorpus).\n",
    "- Fine-tuning the pre-trained model on a smaller, task-specific dataset.\n",
    "\n",
    "### Advantages\n",
    "- **Reduces the need for large amounts of labeled data** for each new task.\n",
    "- **Speeds up training** and improves performance on specialized tasks.\n",
    "- **Enables rapid adaptation** to new domains or languages with minimal data.\n",
    "\n",
    "Transfer learning has become a cornerstone of modern NLP, enabling state-of-the-art results across a wide range of applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# load RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True,)\n",
    "\n",
    "# prepare datset \n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"text\")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 3,\n",
    "    weight_decay = 0.01,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()\n",
    "\n",
    "# evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# print evaluation results\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539822e",
   "metadata": {},
   "source": [
    "Use GPT for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fba431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api key(fake)\n",
    "openai.api_key = \"ca-pfnubap-W8NRg20bikndufh3-28rb3onf92h-b1oe1_b3fubifna\"\n",
    "\n",
    "try: \n",
    "    # genereate text using 3.5 Turbo\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"Write  short story about a robot learning to cook.\"}],\n",
    "        max_tokens = 150,\n",
    "        temperature = 0.7\n",
    "    )\n",
    "\n",
    "    print(\"Generated Text:\\n\" response[\"choices\"][0][\"message\"][\"content\"].strip())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
