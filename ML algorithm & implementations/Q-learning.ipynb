{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d62ac1f",
   "metadata": {},
   "source": [
    "# Q-Learning Implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22195b3",
   "metadata": {},
   "source": [
    "#### Q-Learning\n",
    "Q-Learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given problem. It learns by interacting with an environment, updating a Q-table (a matrix of state-action values), and maximising the expected cumulative reward. Q-learning is effective in problems where the environment can be represented by discrete states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d6322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      " [[0.21934498 0.59049    0.08275192 0.05310723]\n",
      " [0.46143098 0.6561     0.44141008 0.21548598]\n",
      " [0.59995453 0.729      0.53141762 0.52582009]\n",
      " [0.68937286 0.61846349 0.81       0.62176308]\n",
      " [0.53143705 0.13719023 0.04959375 0.12444136]\n",
      " [0.59049    0.33903361 0.09846198 0.29278765]\n",
      " [0.6561     0.3317031  0.22439345 0.        ]\n",
      " [0.65674792 0.7675824  0.9        0.53017256]\n",
      " [0.47478692 0.16422346 0.03287078 0.00638288]\n",
      " [0.53144098 0.18275593 0.04858476 0.14429141]\n",
      " [0.59048986 0.46952517 0.04760709 0.09087629]\n",
      " [0.73023335 0.76490632 1.         0.44825812]\n",
      " [0.04128297 0.42807872 0.         0.04041451]\n",
      " [0.47828594 0.15508944 0.05428075 0.08015191]\n",
      " [0.53081192 0.19       0.         0.08177904]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Learned policy:\n",
      " [[1 1 1 2]\n",
      " [0 0 0 2]\n",
      " [0 0 0 2]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# define the environment (4x4 grid)\n",
    "num_states = 16 # 4x4 grid\n",
    "num_actions = 4 # up, down, left, right\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# define the paramaeters\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "epsilon = 0.1 # exploration rate\n",
    "num_episodes = 1000 # number of episodes\n",
    "\n",
    "# define a simple reward structure\n",
    "rewards = np.zeros(num_states)\n",
    "rewards[15] = 1\n",
    "\n",
    "# functon to determine the next state based on the action\n",
    "def get_next_state(state,action):\n",
    "    if action == 0 and state >=4: # up\n",
    "        return state - 4\n",
    "    elif action == 1 and (state + 1) % 4 !=0: # right\n",
    "        return state + 1\n",
    "    elif action == 2 and state < 12: # down\n",
    "        return state + 4\n",
    "    elif action == 3 and state % 4 != 0: # left\n",
    "        return state - 1\n",
    "    else:\n",
    "        return state # if action goes out of bounds, stay in the same state\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = random.randint(0,num_states-1) # start in a random state\n",
    "    while state!=15: # loop until reaching the goal state\n",
    "        if random.uniform(0,1) < epsilon: # explore\n",
    "            action = random.randint(0,num_actions-1)\n",
    "        else: # exploit\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        next_state = get_next_state(state,action)\n",
    "        reward = rewards[next_state]\n",
    "        old_value = q_table[state,action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        # Q-Learning update rule\n",
    "        new_value = old_value + alpha*(reward + gamma*next_max - old_value)\n",
    "        q_table[state,action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "# display the learned Q-table\n",
    "print(\"Learned Q-table:\\n\",q_table)\n",
    "\n",
    "# visualise the learned policy\n",
    "policy = np.argmax(q_table, axis=1)\n",
    "policy = policy.reshape(4,4)\n",
    "print(\"Learned policy:\\n\",policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
