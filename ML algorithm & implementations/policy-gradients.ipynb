{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ee788a",
   "metadata": {},
   "source": [
    "# Policy Gradients Implementations in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1266a",
   "metadata": {},
   "source": [
    "#### Policy Gradient Methods\n",
    "Policy Gradient Methods are a class of reinforcement learning algorithms that learn a policy directly by optimising the parameters of a policy network. Instead of learning Q-values like Q-learning or DQN, policy gradient methods focus on finding the optimal action-selection strategy that maximises cumulative rewards. A popular approach is the REINFORCE algorithm, where actions are sampled from a policy distribution, and the policy is updated using gradients based on rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     64\u001b[39m     action = choose_action(state)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     next_state, reward, done, truncated, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     done = \u001b[38;5;28mbool\u001b[39m(done \u001b[38;5;129;01mor\u001b[39;00m truncated)\n\u001b[32m     67\u001b[39m     done = done \u001b[38;5;129;01mor\u001b[39;00m truncated \u001b[38;5;66;03m# end the episode if truncated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[39m, in \u001b[36mTimeLimit.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m     40\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[32m     41\u001b[39m \n\u001b[32m     42\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m._elapsed_steps += \u001b[32m1\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._elapsed_steps >= \u001b[38;5;28mself\u001b[39m._max_episode_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mself\u001b[39m.checked_step = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.step(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233\u001b[39m, in \u001b[36menv_step_passive_checker\u001b[39m\u001b[34m(env, action)\u001b[39m\n\u001b[32m    230\u001b[39m obs, reward, terminated, truncated, info = result\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool8\u001b[49m)):\n\u001b[32m    234\u001b[39m     logger.warn(\n\u001b[32m    235\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    236\u001b[39m     )\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np.bool8)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyrus\\Documents\\AI Mastery\\venv\\Lib\\site-packages\\numpy\\__init__.py:427\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[34m__name__\u001b[39m, attr))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import gym\n",
    "\n",
    "# set up the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99  # discount factor\n",
    "\n",
    "\n",
    "# policy network\n",
    "def build_policy_model():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(24, activation=\"relu\", input_shape=(state_shape,)),\n",
    "            layers.Dense(24, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "policy_model = build_policy_model()\n",
    "\n",
    "# functon to select an action based on policy\n",
    "def choose_action(state):\n",
    "    state = np.array(state).reshape([1, state_shape]) # reshape state to (1, state_shape)\n",
    "    probabilities = policy_model.predict(state)\n",
    "    return np.random.choice(num_actions, p=probabilities[0])\n",
    "    \n",
    "# function to calculate returns (discounted rewards)\n",
    "def discount_rewards(rewards):\n",
    "    discounted = np.zeros_like(rewards)\n",
    "    cumulative = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        cumulative = cumulative * gamma + rewards[i]\n",
    "        discounted[i] = cumulative\n",
    "    return discounted - np.mean(discounted) # normalize\n",
    "\n",
    "# training function\n",
    "def train_on_episode(states, actions, rewards):\n",
    "    discounted_rewards = discount_rewards(rewards)\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs = policy_model(tf.convert_to_tensor(states, dtype=tf.float32), training=True)\n",
    "        action_indices = tf.stack([tf.range(len(actions)),actions], axis=1)\n",
    "        selected_action_probs = tf.gather_nd(action_probs, action_indices)\n",
    "        loss = -tf.reduce_mean(tf.math.log(selected_action_probs) * discounted_rewards)\n",
    "    gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    policy_model.optimizer.apply_gradients(zip(gradients, policy_model.trainable_variables))\n",
    "\n",
    "# main training loop\n",
    "num_episodes = 100\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episodes_states, episodes_actions, episodes_rewards = [], [], []\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated # end the episode if truncated\n",
    "        episodes_states.append(state)\n",
    "        episodes_actions.append(action)\n",
    "        episodes_rewards.append(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            episodes_states = np.vstack(episodes_states)\n",
    "            train_on_episode(episodes_states, np.array(episodes_actions), np.array(episodes_rewards))\n",
    "            print(f\"Episode: {episode+1}/{num_episodes}, Reward: {np.sum(episodes_rewards)}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# conflict between numpy version and other libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
