{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e44efa",
   "metadata": {},
   "source": [
    "### Transfew Learning in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad2431",
   "metadata": {},
   "source": [
    "### Popular Pre-Trained NLP Models\n",
    "\n",
    "#### **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "- **Architecture:** Transformer-based encoder model\n",
    "- **Training Tasks:**\n",
    "    - **Masked Language Modeling (MLM):** Randomly masks words in a sentence and trains the model to predict them.\n",
    "    - **Next Sentence Prediction (NSP):** Trains the model to predict if one sentence follows another.\n",
    "- **Applications:**\n",
    "    - Text classification\n",
    "    - Sentiment analysis\n",
    "    - Question answering\n",
    "    - Named entity recognition (NER)\n",
    "    - Sentence similarity\n",
    "\n",
    "---\n",
    "\n",
    "#### **GPT (Generative Pretrained Transformer)**\n",
    "- **Architecture:** Transformer-based decoder model\n",
    "- **Training Task:**\n",
    "    - **Causal Language Modeling:** Predicts the next word in a sequence (unidirectional).\n",
    "- **Applications:**\n",
    "    - Text generation\n",
    "    - Summarization\n",
    "    - Dialogue systems (chatbots)\n",
    "    - Code generation\n",
    "    - Creative writing\n",
    "\n",
    "---\n",
    "\n",
    "#### **T5 (Text-to-Text Transfer Transformer)**\n",
    "- **Approach:** Treats every NLP problem as a text-to-text task (input and output are always text).\n",
    "- **Applications:**\n",
    "    - Summarization\n",
    "    - Translation\n",
    "    - Text classification\n",
    "    - Question answering\n",
    "    - Sentence paraphrasing\n",
    "\n",
    "---\n",
    "\n",
    "#### **RoBERTa (Robustly Optimized BERT Approach)**\n",
    "- **Improvements over BERT:**\n",
    "    - Removes Next Sentence Prediction (NSP) task.\n",
    "    - Trained on larger datasets with longer sequences and more robust training strategies.\n",
    "- **Applications:**\n",
    "    - Similar to BERT but often achieves better performance on downstream tasks such as classification, NER, and QA.\n",
    "\n",
    "---\n",
    "\n",
    "### Tokenization and Text Preprocessing for Fine-Tuning NLP Models\n",
    "\n",
    "#### **Tokenization**\n",
    "- Converts raw text into numerical representations (tokens) that models can process.\n",
    "- **Types:**\n",
    "    - **WordPiece Tokenization:** Used in BERT; splits words into subword units.\n",
    "    - **Byte Pair Encoding (BPE):** Used in GPT and RoBERTa; merges frequent pairs of characters or subwords.\n",
    "\n",
    "#### **Text Preprocessing**\n",
    "- **Cleaning:**\n",
    "    - Remove unnecessary characters (e.g., URLs, special symbols, HTML tags).\n",
    "    - Normalize text (convert to lowercase, remove extra spaces).\n",
    "- **Optional Steps:**\n",
    "    - Remove stopwords (common words like \"the\", \"is\", etc., if not needed for the task).\n",
    "    - Lemmatization or stemming (reduce words to their base form).\n",
    "- **Tokenization:**\n",
    "    - Break text into tokens compatible with the chosen pre-trained model.\n",
    "\n",
    "---\n",
    "\n",
    "### Adapting Pre-Trained Models for NLP Tasks\n",
    "\n",
    "#### **Common Tasks**\n",
    "- **Text Classification:** Categorize text into predefined labels (e.g., spam detection, topic classification).\n",
    "- **Sentiment Analysis:** Determine the sentiment polarity (positive, negative, neutral) of text.\n",
    "- **Summarization:** Generate concise summaries from lengthy texts.\n",
    "- **Named Entity Recognition (NER):** Identify entities like names, locations, and organizations in text.\n",
    "- **Question Answering:** Extract answers from context passages.\n",
    "\n",
    "#### **Fine-Tuning Steps**\n",
    "1. **Load Pretrained Model:** Choose a model architecture and load pretrained weights.\n",
    "2. **Add a Task-Specific Head:** Attach a classification, regression, or sequence labeling head as needed.\n",
    "3. **Prepare Data:** Tokenize and preprocess your dataset according to the model's requirements.\n",
    "4. **Fine-Tune Model:** Train the model on your specific dataset, adjusting weights for your task.\n",
    "5. **Evaluate and Deploy:** Assess performance on validation/test data and deploy the model for inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad3fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, paddding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\"text\")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class= tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16616b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess_t5(examples):\n",
    "    input = [\"classify sentiment: \"+ doc for doc in examples [\"text\"]]\n",
    "    model_inputs = tokenizer(input, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    labels = tokenizer(examples[\"label\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_t5 = dataset.map(preprocess_t5, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_t5[\"train\"],\n",
    "    eval_dataset=tokenized_t5[\"test\"],\n",
    "    processing_class= tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
