{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4359ce",
   "metadata": {},
   "source": [
    "### Domain Adaption and Transfer Learning Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87435d1",
   "metadata": {},
   "source": [
    "## Understanding Domain Adaptation and Handling Domain-Specific Data\n",
    "\n",
    "### What is Domain Adaptation?\n",
    "\n",
    "Domain adaptation involves transferring a model trained on one domain (**source domain**) to perform tasks in a different domain (**target domain**).\n",
    "\n",
    "**Example:**\n",
    "- **Source domain:** General news articles\n",
    "- **Target domain:** Medical text\n",
    "\n",
    "### Why Domain Adaptation?\n",
    "\n",
    "- Many real-world tasks involve domain-specific data.\n",
    "- Pre-trained models on general datasets might not perform optimally without adaptation.\n",
    "- Domain adaptation helps leverage existing models and data, reducing the need for large labeled datasets in the target domain.\n",
    "\n",
    "### Steps in Domain Adaptation\n",
    "\n",
    "1. **Fine-tune the pre-trained model** on a domain-specific dataset.\n",
    "2. **Incorporate domain-specific embeddings or vocabulary** to better capture the nuances of the target domain.\n",
    "3. **Apply additional pre-training** on the domain data if necessary, to further align the model with the target domain's characteristics.\n",
    "\n",
    "### Challenges in Transfer Learning\n",
    "\n",
    "#### Data Mismatch\n",
    "- The source domain may not represent the target domain adequately.\n",
    "- **Example:** General text (Wikipedia) vs. technical medical jargon.\n",
    "\n",
    "#### Catastrophic Forgetting\n",
    "- During fine-tuning, the model may forget the knowledge learned from the source domain, leading to reduced generalization.\n",
    "\n",
    "#### Computational Constraints\n",
    "- Fine-tuning large pre-trained models requires significant computational resources, which may not always be available.\n",
    "\n",
    "### Strategies to Address Challenges\n",
    "\n",
    "#### Transfer Learning from Related Domains\n",
    "- Fine-tune on an intermediate domain dataset before adapting to the target domain.\n",
    "- **Example:** Adapt from general news to scientific articles, then to medical text.\n",
    "\n",
    "#### Data Augmentation\n",
    "- Generate synthetic domain-specific data to augment the target dataset.\n",
    "- **Example:** Use paraphrasing techniques or back-translation to increase dataset diversity.\n",
    "\n",
    "#### Domain-Specific Embeddings\n",
    "- Use pre-trained embeddings tailored to the target domain (e.g., BioWordVec for biomedical text).\n",
    "- Incorporate specialized tokenizers or vocabularies to better handle domain-specific terminology.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "Domain adaptation is essential for applying machine learning models to specialized domains. By understanding the challenges and employing effective strategies, we can improve model performance and generalization in domain-specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a795484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pubmed 20k RCT dataset\n",
    "dataset = load_dataset(\"pubmed_rct\", \"20k_rct\")\n",
    "pritn(dataset[\"train\"][0])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    return tokenizer(examples[\"abstract\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_data, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dmis-lab/biobert-base-cased-v1.1\", num_labels=5\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# evaluate model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "import random\n",
    "\n",
    "def augment_text(text):\n",
    "    synonyms = {\"cancer\": [\"tumour\", \"tumours\"], \"therapy\": [\"treatment\", \"treatments\"]}\n",
    "    words = text.split()\n",
    "    new_words = [random.choice(synonms[word]) if word in synonyms else word for word in words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "augmented_data = [augmented_text(sample[\"text\" ]) for sample in dataset[\"train\"]]\n",
    "\n",
    "augmented_dataset = dataset[\"train\"].add_column(\"augmented_text\", augmented_data)\n",
    "\n",
    "augmented_tokenized_dataset = augmented_dataset.map(preprocess_data, batched=True)\n",
    "augmented_tokenized_dataset = augmented_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "augmented_tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "augmented_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=augmented_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=augmented_tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "augmented_trainer.train()\n",
    "\n",
    "augmented_results = augmented_trainer.evaluate()\n",
    "print(augmented_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
