{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bdedf1",
   "metadata": {},
   "source": [
    "### Fine Tuning Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d207e7",
   "metadata": {},
   "source": [
    "### Discriminative Fine Tuning\n",
    "\n",
    "- Different layers of a pre-trained model capture different types of information.\n",
    "- **Approach:**  \n",
    "    - Use different learning rates for different layers of the model.\n",
    "    - Lower learning rates for early layers (which capture general features).\n",
    "    - Higher learning rates for later layers (which capture task-specific features).\n",
    "- **Benefit:**  \n",
    "    - Allows fine-tuning to adapt higher layers to the new task while preserving useful representations in lower layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Slanted Triangular Learning Rates (SLTR)\n",
    "\n",
    "- Dynamically adjusts learning rates during training to balance exploration and convergence.\n",
    "- **Phases:**\n",
    "    - **Warm-up:** Gradually increase the learning rate to promote exploration and avoid early convergence to poor minima.\n",
    "    - **Decay:** Slowly decrease the learning rate to ensure stable convergence.\n",
    "- **Use Case:**  \n",
    "    - Particularly effective for fine-tuning pre-trained models like BERT and GPT, where careful learning rate scheduling can prevent catastrophic forgetting and improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization and Dropout for Preventing Overfitting in NLP Models\n",
    "\n",
    "- **Regularization:**\n",
    "    - **L1 Regularization:** Encourages sparsity by penalizing the absolute values of weights, leading to some weights becoming zero.\n",
    "    - **L2 Regularization (Ridge):** Penalizes large weights by adding the squared magnitude of weights to the loss function, helping to prevent overfitting.\n",
    "- **Dropout:**\n",
    "    - Randomly drops units (neurons) during training.\n",
    "    - Prevents over-reliance on specific neurons, encouraging the model to learn more robust features.\n",
    "    - Commonly used in transformer-based models and other deep learning architectures.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluating Model Performance with NLP-Specific Metrics\n",
    "\n",
    "- **Key Metrics:**\n",
    "    - **F1 Score:**\n",
    "        - Harmonic mean of precision and recall.\n",
    "        - Suitable for classification tasks, especially with imbalanced datasets.\n",
    "    - **BLEU Score:**\n",
    "        - Evaluates the quality of generated text against reference text.\n",
    "        - Commonly used for machine translation and text summarization tasks.\n",
    "    - **ROUGE Score:**\n",
    "        - Measures overlap between generated and reference summaries.\n",
    "        - Widely used for evaluating summarization models.\n",
    "    - **Perplexity:**\n",
    "        - Measures how well a language model predicts a sample.\n",
    "        - Lower perplexity indicates better performance for language modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec27aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea553511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"train\"][\"text\"], dataset[\"train\"][\"label\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_data(texts, labels, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "train_data = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "test_data = tokenize_data(test_texts, test_labels, tokenizer)\n",
    "\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input_ids = data[\"input_ids\"]\n",
    "        self.attention_mask = data[\"attention_mask\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[index]),\n",
    "            \"attention_mask\": torch.tensor(self.attention_mask[index]),\n",
    "            \"labels\": torch.tensor(self.labels[index]),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ber-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# define optimiser and STLR scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# define STLR scheduler\n",
    "num_training_steps = len(train_dataloader) * 3\n",
    "warmup_steps = int(0.1 * num_training_steps)\n",
    "scheduler = get_scheduler(\n",
    "    \"slanted_triangular\",\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# training loop\n",
    "device = torch.device(\"cuda\"  torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        for batch in train_dataloader:\n",
    "            batch = (k: v.to(device) for k, v in batch.items())\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "train_model()\n",
    "\n",
    "# evaluate model using f1 score\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = (k: v.to(device) for k, v in batch.items())\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "\n",
    "from sacredbleu import BLEU\n",
    "\n",
    "# evaluate model using BLEU score\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = (k: v.to(device) for k, v in batch.items())\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "bleu = BLEU(all_labels, all_preds)\n",
    "print(f\"BLEU score: {bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
